# Stage 4: Creating a python algorithm that accepts varying data from CSVs

The motivation for this stage is to transition away from hardcoded scripts that only accept CSV files with a fixed structure, including specific column numbers and names. 

_Please note that I am still using CSVs because the data I currently have is stored in CSV format. The code takes these CSVs and converts them into a Python list. In the future, automated machines will only need to output a Python list with the necessary information for integration._

This code is designed with the following goals in mind:

1. **Data Input**: The script takes in data from the automated experiment as a Python list.
2. **Node and Relationship Generation**: It dynamically generates the appropriate nodes and relationships based on the type of operation.
3. **Flexible Node Properties**: Apart from a few required properties needed to identify and create nodes/relationships, the system allows node properties to be filled with any data.
4. **Real-Time Generation**: Nodes and relationships can be generated in real time or shortly after the experiment, without the need to re-upload the entire database.

## 4.1: Basic Structure of the Script

The [Jupyter Notebook](https://github.com/lteresah/AMLS-GraphDatabase/blob/main/Stage4/neo4jPythonAPI_DynamicCSV.ipynb) provided populates the database following these steps:

1. Reads in csv files from the internet and turns each into a pandas dataframe
2. Takes each pandas dataframe associated with an Operation Node, extracts the data from the able, and creates a chronologically ordered **Operation Log** which takes the form of a python list.

    The list has the following structure:
  
    | Operation Type (string) | Timestamp (datetime) | User (string) |  Operation ID (string) | Data (dict) |
    |:--------------:|:---------:|:----:|:-------------:|:----:|
    | 'Mix' |'2024-03-13T12:00:00'|'lteresa'| 'MIX-2024-03-13T12:00:00-lteresa:01-314024and01-334090at20C| {'Mix Timestamp': '2024-03-13T12:00:00',  'Executor': 'lteresa', 'Process Owner': 'dasb', 'Mix Operation ID': 'MIX-2024-03-13T12:00:00-lteresa:01-314024and01-334090at20C', 'Mix Temp (C)': 20, 'Dissolved': 'Y', 'New Object ID': '02-000001', 'New Object Type': 'Precursor', 'New Object Description': '1M CsCH3COO in HBr', ...}|
    | ... | ... | ... | ... | ... | 

3. Establishes connection to a Neo4j Database.
4. Performs a sophisticated process to create nodes and relationships in the database.

## 4.2: More Detailed Information on Node and Relationship Generation

### 4.2.1: Data Input

The script takes in two types of csv files:
1. Files associated with **objects** -- assumed to be independently maintained by some other system.
   - Example:
       - List of Users and User Information
       - List (Stock) of Ingredients and Ingredient Information
2. Files associated with **operations** -- manually recorded by a User or automatically outputed by an experiment

The files are originally read and stored as pandas dataframes. Operation Type files are collected and converted into a single python list (as described in the previous section), while Object Type files are eventually transformed into a dictionary list for node generation.

### 4.2.2: Defining the Model

To manage entropy, a Python dictionary is defined early on to specify the types of nodes that can be created in the database. This dictionary includes important information about each node type and guides functions on how to process data associated with these nodes.

The dictionary defines two _types_ of nodes (for now):
1. Objects
2. Operations

There are several nodes within each type, each with their own defined
1. Name
2. Unique Identifier (UID)
   - UID here refers to the _name of the column_ that is used for the unique identifier, not the value.
4. Functions
   - Tells the system what kind of role each node plays. Helps functions know what information to look for, what kind of nodes to create, and how to connect associated nodes.
   - The functions included in this database are:
     1. person
     2. item
     3. item_consuming
     4. item_acting
     5. item_creating
    -  May be a good idea to replace 'item' with 'object' and consolidate 'person' into 'object'.

### 4.2.3: Dynamic Generation of Cypher Scripts, Nodes, and Relationshps

Cypher scripts are dynamically generated by functions which output them as python strings.  There is one function for each of these different types of queries:
1. gencypher_createConstraint
2. gencypher_createObjectNode
3. gencypher_createOperationNode
4. gencypher_createRelations

The cypher functions are embedded inside other functions that directly manipulate the database.  These functions are executed in the following order:

1. db_setConstraints() : Create Constraints for DB nodes.
   - Scans node dictionary
   - Generates cypher script to create constraint for each node
   - Passes that script into the Neo4j database as a transaction
2. db_pushObjects() : Creates Object Nodes
   - Reads the pandas data structures associated with Users and Ingredients
   - Generates cypher script to create object nodes
   - Passes that script into the Neo4j database as a transaction
3. db_pushOperations(): Creates Operation Nodes and their associated Relationships
   - Iterates through the Operation_Log list
   - Generates cypher script to create operation nodes
   - Generates cypher scripts to connect operation nodes to relevant objects
   - Combines above cypher scripts
   - Passes final script into the Neo4j database as a transaction
   
## 4.3: Missing Features, Lessons Learned, and Recommended Next Steps

### 4.3.1: Missing Features

1. This script does _not_ populate the properties of nodes that are created within an operation (for example: a mix node).  While the produce node, itself, is created within the db_pushOperations() function, only the UID property is filled in. There is currently no distinguisher for properties that go to the operation node and those that go to the product node(s). Instead, all the properties of an operation go to the operation node.
   
2. Although it would likely produce an error if you tried to create a nodetype not in the Node Dictionary, the script does not explicitly check if a node is allowed when creating it.  This opens up avenues for unregistered nodes to be created, especially during the db_PushOperations() stage.
   
4. The db_PushObjects() function only works for pandas structures, as it was designed to load Users and Ingredients. However, other types of objects exist, and it would be nice if the function were general enough to handle all kinds of objects.

### 4.3.2: Lessons Learned

#### Lesson 4.1: Dynamic reading of required properties is difficult when they're all named differently. Enforce a common name for the required properties of nodes.
   - The column name for the UID in each of the Operators was named differently. They were: Mix Operation ID, Heat Operation ID, and Rest Operation ID. This caused some difficulties when trying to extract the value of those IDs dynamically, requiring the use of some regex-like functions.
   - I ran into the same problem with timestamps, as the columns were named similarly.
   - These properties should simply be referred to as 'Operation ID', 'Timestamp', etc.
  
#### Lesson 4.2: Enforcing the first column of the CSV to be node name will simplify the code.

We intend to move away from CSVs, so this might not matter. However, the same applies when automatically generating lists during experimentation.

### 4.3.3: Recommended Next Steps

1. Add the missing features mentioned in section 4.3.1
2. Enforce common names for required properties and require Operation (or node) type be the first column in the CSV.
3. Modify the script to accomodate the above changes.
4. Create modules or packages out of the functions in the script for easier implementation.
5. Create a script which _similulates_ data to test the limitations of this method, especially with larger amounts of data.
